{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "background_execution": "on"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoM7IjsMn_PE"
      },
      "outputs": [],
      "source": [
        "#ライブラリ\n",
        "import math\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "from pycocotools.coco import COCO\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "import torch.utils.data as data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#COCOデータダウンロード\n",
        "!mkdir data\n",
        "!wget http://msvocds.blob.core.windows.net/annotations-1-0-3/captions_train-val2014.zip -P ./data/\n",
        "!wget http://images.cocodataset.org/zips/train2014.zip -P ./data/\n",
        "\n",
        "!unzip ./data/captions_train-val2014.zip -d ./data/\n",
        "!rm ./data/captions_train-val2014.zip\n",
        "!unzip ./data/train2014.zip -d ./data/\n",
        "!rm ./data/train2014.zip "
      ],
      "metadata": {
        "id": "1V5HSXVeoTq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ボキャブラリー作成\n",
        "class Vocabulary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if not word in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            self.idx += 1\n",
        "\n",
        "    def __call__(self, word):\n",
        "        if not word in self.word2idx:\n",
        "            return self.word2idx['<unk>']\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)\n",
        "\n",
        "def make_vocab(json, threshold):\n",
        "    coco = COCO(json)\n",
        "    counter = Counter()\n",
        "    ids = coco.anns.keys()\n",
        "    for i, id in enumerate(ids):\n",
        "        caption = str(coco.anns[id]['caption'])\n",
        "        tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
        "        counter.update(tokens)\n",
        "\n",
        "        if (i+1) % 1000 == 0:\n",
        "            print(\"[{}/{}] Tokenized captions.\".format(i+1, len(ids)))\n",
        "\n",
        "    words = [word for word, cnt in counter.items() if cnt >= threshold]\n",
        "\n",
        "    vocab = Vocabulary()\n",
        "    vocab.add_word('<pad>')\n",
        "    vocab.add_word('<start>')\n",
        "    vocab.add_word('<end>')\n",
        "    vocab.add_word('<unk>')\n",
        "\n",
        "    for i, word in enumerate(words):\n",
        "        vocab.add_word(word)\n",
        "    return vocab\n",
        "\n",
        "vocab = make_vocab(json='./data/annotations/captions_train2014.json', threshold=4)\n",
        "vocab_path = './data/vocab.pkl'\n",
        "with open(vocab_path, 'wb') as f:\n",
        "   pickle.dump(vocab, f)\n",
        "print(\"Total vocabulary size: {}\".format(len(vocab)))\n",
        "print(\"Saved vocabulary wrapper to '{}'\".format(vocab_path))"
      ],
      "metadata": {
        "id": "L3Dpjg6noWYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#COCO画像リサイズ\n",
        "def resize_image(image, size):\n",
        "    return image.resize((size,size), Image.ANTIALIAS)\n",
        "\n",
        "def resize_images(image_dir, output_dir, size):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    images = os.listdir(image_dir)\n",
        "    num_images = len(images)\n",
        "    for i, image in enumerate(images):\n",
        "        with open(os.path.join(image_dir, image), 'r+b') as f:\n",
        "            with Image.open(f) as img:\n",
        "                img = resize_image(img, size)\n",
        "                img.save(os.path.join(output_dir, image), img.format)\n",
        "        if (i+1) % 100 == 0:\n",
        "            print (\"[{}/{}] Resized the images and saved into '{}'.\"\n",
        "                   .format(i+1, num_images, output_dir))\n",
        "\n",
        "image_dir = './data/train2014/'\n",
        "output_dir = './data/resized2014/'\n",
        "image_size = 256\n",
        "resize_images(image_dir, output_dir, image_size)"
      ],
      "metadata": {
        "id": "HA-5qcNtonlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#COCOカスタムデータセット\n",
        "class CocoDataset(data.Dataset):\n",
        "    def __init__(self, root, json, vocab, transform=None):\n",
        "        self.root = root\n",
        "        self.coco = COCO(json)\n",
        "        self.ids = list(self.coco.anns.keys())\n",
        "        self.vocab = vocab\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        coco = self.coco\n",
        "        vocab = self.vocab\n",
        "        ann_id = self.ids[index]\n",
        "        caption = coco.anns[ann_id]['caption']\n",
        "        img_id = coco.anns[ann_id]['image_id']\n",
        "        path = coco.loadImgs(img_id)[0]['file_name']\n",
        "\n",
        "        image = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
        "        caption = []\n",
        "        caption.append(vocab('<start>'))\n",
        "        caption.extend([vocab(token) for token in tokens])\n",
        "        caption.append(vocab('<end>'))\n",
        "        target = torch.Tensor(caption)\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)"
      ],
      "metadata": {
        "id": "H7OAhApSotOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#バッチ\n",
        "def collate_fn(data):\n",
        "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
        "    images, captions = zip(*data)\n",
        "\n",
        "    images = torch.stack(images, 0)\n",
        "\n",
        "    lengths = [len(cap) for cap in captions]\n",
        "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
        "    for i, cap in enumerate(captions):\n",
        "        end = lengths[i]\n",
        "        targets[i, :end] = cap[:end]        \n",
        "    return images, targets\n",
        "\n",
        "def get_loader(root, json, vocab, transform, batch_size, shuffle, num_workers):\n",
        "    coco = CocoDataset(root=root,\n",
        "                       json=json,\n",
        "                       vocab=vocab,\n",
        "                       transform=transform)\n",
        "    \n",
        "    data_loader = torch.utils.data.DataLoader(dataset=coco, \n",
        "                                              batch_size=batch_size,\n",
        "                                              shuffle=shuffle,\n",
        "                                              num_workers=num_workers,\n",
        "                                              collate_fn=collate_fn,\n",
        "                                              drop_last=True)\n",
        "    \n",
        "    return data_loader"
      ],
      "metadata": {
        "id": "EuESJoz1owrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, hidden_size, NM=4):\n",
        "        super(Encoder, self).__init__()\n",
        "        resnet = models.resnet152(pretrained=True)\n",
        "        modules = list(resnet.children())[:-2]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        self.attn_conv = nn.Conv2d(2048, NM, 1, bias=False)\n",
        "        nn.init.xavier_uniform_(self.attn_conv.weight)\n",
        "        self.linear = nn.Linear(resnet.fc.in_features*NM, hidden_size)\n",
        "        self.ln = nn.LayerNorm(hidden_size)\n",
        "        self.NM = NM\n",
        "\n",
        "    def forward(self, images):\n",
        "        with torch.no_grad():\n",
        "            features = self.resnet(images)\n",
        "        atten_map = torch.sigmoid(self.attn_conv(features))\n",
        "        B, _, H, W = atten_map.shape\n",
        "        features = features.reshape(B, 1, 2048, H, W)\n",
        "        atten_map = atten_map.reshape(B, self.NM, 1, H, W)\n",
        "        features = features * atten_map\n",
        "        features = features.reshape(B*self.NM, 2048, H, W)\n",
        "        features = F.adaptive_avg_pool2d(features, (1, 1))\n",
        "        features = features.reshape(B, -1)\n",
        "        features = self.ln(self.linear(features))\n",
        "        return features"
      ],
      "metadata": {
        "id": "Dd6CFjTL_BfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, batch_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.cell = torch.nn.parameter.Parameter(torch.zeros(num_layers, batch_size, hidden_size))\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.embed_size = embed_size\n",
        "        self.liner_q = nn.Linear(embed_size, embed_size)\n",
        "        self.liner_k = nn.Linear(embed_size, embed_size)\n",
        "        self.liner_v = nn.Linear(embed_size, embed_size)\n",
        "        self.atten_out = nn.Linear(embed_size, embed_size)\n",
        "        \n",
        "    def forward(self, captions, features, padmask=None):\n",
        "        embeddings = self.embed(captions)\n",
        "        embeddings = self.attention(embeddings, embeddings, embeddings, padmask)\n",
        "        features = features.unsqueeze(0)\n",
        "        hiddens, state = self.lstm(embeddings, (features, self.cell))\n",
        "        outputs = self.linear(hiddens)\n",
        "        return outputs, state\n",
        "\n",
        "    def attention(self, q, k, v, padmask=None):\n",
        "        q = self.liner_q(q)\n",
        "        k = self.liner_k(k)\n",
        "        v = self.liner_v(v)\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.embed_size)\n",
        "        if padmask is not None:\n",
        "          scores = scores.masked_fill(padmask == 0, -1e9)\n",
        "        scores = F.softmax(scores, dim=-1)\n",
        "        output = torch.matmul(scores, v)\n",
        "        output = self.atten_out(output)\n",
        "        return output\n",
        "\n",
        "    def decode(self, captions, features, cell, padmask=None):\n",
        "        embeddings = self.embed(captions)\n",
        "        embeddings = self.attention(embeddings, embeddings, embeddings, padmask)\n",
        "        features = features.unsqueeze(0)\n",
        "        hiddens, state = self.lstm(embeddings, (features, cell))\n",
        "        outputs = self.linear(hiddens)\n",
        "        return outputs, state"
      ],
      "metadata": {
        "id": "acTjmL6Vozoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#学習\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_path = 'models/'\n",
        "crop_size = 224\n",
        "vocab_path = 'data/vocab.pkl'\n",
        "image_dir ='data/resized2014'\n",
        "caption_path='data/annotations/captions_train2014.json'\n",
        "log_step=10\n",
        "save_step=1000\n",
        "embed_size=256\n",
        "hidden_size=512\n",
        "num_layers=1\n",
        "num_epochs=10\n",
        "batch_size=64\n",
        "num_workers=2\n",
        "learning_rate=0.001\n",
        "\n",
        "def subsequent_mask(size):\n",
        "    attn_shape = (1, size, size)\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    return torch.from_numpy(subsequent_mask) == 0\n",
        "\n",
        "if not os.path.exists(model_path):\n",
        "    os.makedirs(model_path)\n",
        "\n",
        "transform = transforms.Compose([ \n",
        "    transforms.RandomCrop(crop_size),\n",
        "    transforms.RandomHorizontalFlip(), \n",
        "    transforms.ToTensor(), \n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "with open(vocab_path, 'rb') as f:\n",
        "    vocab = pickle.load(f)\n",
        "\n",
        "data_loader = get_loader(image_dir, caption_path, vocab, transform, batch_size,\n",
        "                          shuffle=True, num_workers=num_workers) \n",
        "\n",
        "encoder = Encoder(hidden_size).to(device)\n",
        "decoder = Decoder(embed_size, hidden_size, len(vocab), num_layers, batch_size).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "params = list(encoder.parameters()) + list(decoder.parameters())\n",
        "optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
        "\n",
        "total_step = len(data_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, captions) in enumerate(data_loader):\n",
        "        \n",
        "        images = images.to(device)\n",
        "\n",
        "        caption = captions[:, :-1]\n",
        "        targets = captions[:, 1:]\n",
        "\n",
        "        caption = caption.to(device)\n",
        "        targets = targets.to(device)\n",
        "        \n",
        "        pad = 0\n",
        "        pad_mask = (targets != pad).unsqueeze(1)\n",
        "        pad_mask = pad_mask & torch.autograd.Variable(subsequent_mask(targets.size(-1)).type_as(pad_mask.data))\n",
        "        \n",
        "        feature = encoder(images)\n",
        "        outputs, state = decoder(caption, feature, pad_mask)\n",
        "\n",
        "        outputs = outputs.reshape(-1, outputs.shape[-1])\n",
        "        targets = targets.reshape(-1)\n",
        "\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        encoder.zero_grad()\n",
        "        decoder.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % log_step == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
        "                  .format(epoch, num_epochs, i, total_step, loss.item(), np.exp(loss.item()))) \n",
        "            \n",
        "        if (i+1) % save_step == 0:\n",
        "            torch.save(encoder.state_dict(), os.path.join(\n",
        "                model_path, 'encoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n",
        "        if (i+1) % save_step == 0:\n",
        "            torch.save(decoder.state_dict(), os.path.join(\n",
        "                model_path, 'decoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n",
        "            \n",
        "    scheduler.step()"
      ],
      "metadata": {
        "id": "-YzGjTnkpFtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#予測\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_path = 'models/'\n",
        "vocab_path = 'data/vocab.pkl'\n",
        "image_dir ='data/resized2014'\n",
        "caption_path='data/annotations/captions_train2014.json'\n",
        "embed_size=256\n",
        "hidden_size=512\n",
        "num_layers=1\n",
        "batch_size=64\n",
        "\n",
        "with open(vocab_path, 'rb') as f:\n",
        "    vocab = pickle.load(f)\n",
        "\n",
        "encoder = Encoder(hidden_size).to(device)\n",
        "decoder = Decoder(embed_size, hidden_size, len(vocab), num_layers, batch_size).to(device)\n",
        "encoder.load_state_dict(torch.load('./models/encoder-10-6000.ckpt',torch.device('cpu')))\n",
        "decoder.load_state_dict(torch.load('./models/decoder-10-6000.ckpt',torch.device('cpu')))\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "test_img_dir = './data/train2014/COCO_train2014_000000000263.jpg'\n",
        "test_img = Image.open(test_img_dir).convert('RGB')\n",
        "test_img = test_img.resize([256, 256], Image.LANCZOS)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(), \n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "test_img = transform(test_img)"
      ],
      "metadata": {
        "id": "tf45GDulpN8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#予測\n",
        "images = test_img.to(device)\n",
        "predicted_ids = []\n",
        "caption = []\n",
        "start_char = [vocab('<start>') for _ in range(batch_size)]\n",
        "input_char = torch.tensor(start_char, device=device)\n",
        "input_char = input_char.unsqueeze(1)\n",
        "images = images.unsqueeze(0)\n",
        "images = images.repeat(batch_size,1,1,1)\n",
        "features = encoder(images)\n",
        "      \n",
        "pad = 0\n",
        "pad_mask = (input_char != pad).unsqueeze(1)\n",
        "pad_mask = pad_mask & (subsequent_mask(input_char.size(-1)).type_as(pad_mask.data))\n",
        "\n",
        "cell = torch.zeros(1, 64, 512).to(device)\n",
        "\n",
        "outputs, states = decoder.decode(input_char, features, cell, pad_mask)\n",
        "state = states[0].squeeze(0)\n",
        "cell = states[1]\n",
        "\n",
        "for i in range(max_length):\n",
        "    with torch.no_grad():\n",
        "        pad = 0\n",
        "        pad_mask = (input_char != pad).unsqueeze(1)\n",
        "        pad_mask = pad_mask & (subsequent_mask(input_char.size(-1)).type_as(pad_mask.data))\n",
        "        outputs, states = decoder.decode(input_char, state, cell, pad_mask)\n",
        "        \n",
        "        state = states[0].squeeze(0)\n",
        "        cell = states[1]\n",
        "\n",
        "        _, output_chars = torch.max(outputs,dim=2)\n",
        "        if int(output_chars[0]) == vocab('<end>'):\n",
        "            break\n",
        "        output_char = output_chars[:,-1]\n",
        "        predicted_ids.append(int(output_char[0]))\n",
        "        input_char = output_char.unsqueeze(1)\n",
        "\n",
        "for j in range(len(predicted_ids)):\n",
        "    word = vocab.idx2word[int(predicted_ids[j])]\n",
        "    caption.append(word)"
      ],
      "metadata": {
        "id": "EGhHZpDht_AB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}